# -*- coding: utf-8 -*-
"""trio_parada_mole.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zp8xmnQs3z2oUsKMGpHiCzDsJWMv1V4F
"""

import pandas as pd

import numpy as np

import sklearn

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from numpy.core.fromnumeric import std
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer

from sklearn import preprocessing

label_encoder = LabelEncoder()

df = pd.read_csv('abalone_dataset.csv')

#df = pd.read_csv('abalone_app.csv')

df.head()

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
print(df)

features = ['sex', 'length', 'diameter', 'height', 'whole_weight', 'shucked_weight', 'viscera_weight', 'shell_weight', 'type']

df = df[features]

df.describe()

df.info()

sex_mapping = {'M': 0, 'F': 1, 'I': 2}
df['sex'] = df['sex'].map(sex_mapping)
df.info()
X = df.drop('type', axis=1)  # Exclua a coluna 'type' para obter as características
y = df['type']

#média das colunas
#mean_Columns = ['sex', 'length', 'diameter', 'height', 'whole_weight', 'shucked_weight', 'viscera_weight', 'shell_weight', 'type']

#mean = df[mean_Columns].mean()
#print(mean)
mean = df.mean()
print(mean)

#std
#std_Columns = ['sex', 'length', 'diameter', 'height', 'whole_weight', 'shucked_weight', 'viscera_weight', 'shell_weight', 'type']

#std = df[std_Columns].std()
#print(std)
stdl = df.std()
print(stdl)

#detectando os outliers com zscore
#threshold = 3
#mean = df.mean()
#std = df.std()

#z_scores = (df - mean) / std

#outliers_mask = z_scores > threshold

#outliers_values = df[outliers_mask]

#print('Outliers in the dataset are:')
#print(outliers_values)

# Cria o objeto SimpleImputer para imputar a moda
imputer = SimpleImputer(strategy='most_frequent')

# Aplica a imputação na coluna 'YearBuilt'
X_col_imputed = imputer.fit_transform(df['sex'].values.reshape(-1, 1))

# substitui os valores faltantes pela média imputada
df['sex'] = X_col_imputed

df.head()

# seleciona as colunas que deseja padronizar
cols_to_scale = [ 'length', 'diameter', 'height', 'whole_weight', 'shucked_weight', 'viscera_weight', 'shell_weight', 'type']

# Criar um objeto StandardScaler
scaler = StandardScaler()

# aplica o StandardScaler nas colunas selecionadas
df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])

# Exibir o DataFrame após a normalização
df.head()



# Separar a variável de saída das variáveis de entrada
X = df.drop('type', axis=1)
y = df['type']
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)
# Dividir os dados em conjuntos de treinamento e teste
X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.3, random_state=42)

# Exibir as dimensões dos conjuntos de treinamento e teste
print('Conjunto de treinamento: ', X_treino.shape, y_treino.shape)
print('Conjunto de teste: ', X_teste.shape, y_teste.shape)

knn = KNeighborsClassifier(n_neighbors=30)
knn.fit(X_train_scaled, y_train_encoded)

# Fazer previsões
y_pred = knn.predict(X_test_scaled)

# Calcular a acurácia
accuracy = accuracy_score(y_test_encoded, y_pred)
print(f"Acurácia: {accuracy}")

df.to_csv('abaloneatt.csv')